{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7c7716-66d7-43b1-85cc-368c5beaa1fa",
   "metadata": {},
   "source": [
    "* Data downloaded from : https://lhncbc.nlm.nih.gov/ii/tools/SemRep_SemMedDB_SKR.html\n",
    "* Data dictionaries (and descs of other tables): https://lhncbc.nlm.nih.gov/ii/tools/SemRep_SemMedDB_SKR/dbinfo.html\n",
    "\n",
    "\n",
    "```\n",
    "PREDICATION table\n",
    "Each record in this table identifies a unique predication. The data fields are as follows:\n",
    "\n",
    "PREDICATION_ID: Auto-generated primary key for each unique predication\n",
    "SENTENCE_ID: Foreign key to the SENTENCE table\n",
    "PMID: The PubMed identifier of the citation to which the predication belongs\n",
    "PREDICATE: The string representation of each predicate (for example TREATS, PROCESS_OF)\n",
    "SUBJECT_CUI: The CUI of the subject of the predication\n",
    "SUBJECT_NAME: The preferred name of the subject of the predication\n",
    "SUBJECT_SEMTYPE: The semantic type of the subject of the predication\n",
    "SUBJECT_NOVELTY: The novelty of the subject of the predication\n",
    "OBJECT_CUI: The CUI of the object of the predication\n",
    "OBJECT_NAME: The preferred name of the object of the predication\n",
    "OBJECT_SEMTYPE: The semantic type of the object of the predication\n",
    "OBJECT_NOVELTY: The novelty of the object of the predication\n",
    "```\n",
    "----------\n",
    "```\n",
    "GENERIC_CONCEPT table\n",
    "This table contains the UMLS Metathesaurus concepts that are considered too generic based upon the 2006AA release. Concepts that are not stored in this table are considered novel. This table is used to populate the SUBJECT_NOVELTY and OBJECT_NOVELTY columns in the PREDICATION table defined below. Data fields in this table are as follows:\n",
    "\n",
    "CONCEPT_ID: Auto generated primary key for each concept\n",
    "CUI: The Concept Unique Identifier (CUI)\n",
    "PREFERRED_NAME: The preferred name of the concept\n",
    "1956C0699748Pathogenesis\n",
    "```\n",
    "\n",
    "\n",
    "* TODO : there are CUIs and concepts with \"|\" in them - e.g. C0034818|3643\tInsulin Receptor|INSR\t , and many genes. How to handle/explode? \n",
    "I ignore them for now and drop them in subsequent steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ba1d74-b20c-424d-a9ca-60efc9281ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SAVE = True\n",
    "FAST = False\n",
    "\n",
    "# SAVE =False\n",
    "# FAST = True\n",
    "\n",
    "FILTER_MIN_PAIR_COOC_THRESHHOLD = 2 ## remove pairs from KG that have less than this many suppoting documents/PMIDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98bbd891-11d0-4d48-9d76-9d282d0de182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.86 s, sys: 275 ms, total: 5.14 s\n",
      "Wall time: 7.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PMID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36840441 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         YEAR\n",
       "PMID         \n",
       "1        1975\n",
       "10       1975\n",
       "100      1975\n",
       "1000     1975\n",
       "10000    1976\n",
       "...       ...\n",
       "9999995  1991\n",
       "9999996  1991\n",
       "9999997  1991\n",
       "9999998  1991\n",
       "9999999  1991\n",
       "\n",
       "[36840441 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### citations - contains PMID and dates/year for things. Useful for tempora l splitting\n",
    "\n",
    "df_cite = pd.read_csv(\"semmedVER43_CITATIONS.csv.gz\",header=None,\n",
    "                      names = [\"PMID\",\"ISSN\",\"DP\",\"EDAT\",\"YEAR\"],usecols=[\"PMID\",\"YEAR\"],\n",
    "                      nrows= 5e6 if FAST else None\n",
    "                     ).set_index(\"PMID\")\n",
    "df_cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133f6a3-612d-494c-992d-f71b93ef3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3/17 min\n",
    "df = pd.read_csv(\"semmedVER43_PREDICATION.csv.gz\", encoding = \"ISO-8859-1\", # try\n",
    "                 nrows=2e5 if FAST else None,\n",
    "                 skip_blank_lines=True,na_values='\\\\N',\n",
    "# usecols = [\n",
    "    # \"PMID\", \"PREDICATE\", \"SUBJECT_CUI\", \"SUBJECT_NAME\", \"SUBJECT_SEMTYPE\", \n",
    "    # \"SUBJECT_NOVELTY\", \"OBJECT_CUI\", \"OBJECT_NAME\", \"OBJECT_SEMTYPE\", \"OBJECT_NOVELTY\"\n",
    "# ]\n",
    " ).dropna(axis=1,how=\"all\") # diff # cols detected\n",
    "\n",
    "df.columns = [\"PREDICATION_ID\",\n",
    "\"SENTENCE_ID\",\n",
    "\"PMID\",\n",
    "\"PREDICATE\",\n",
    "\"SUBJECT_CUI\",\n",
    "\"SUBJECT_NAME\",\n",
    "\"SUBJECT_SEMTYPE\",\n",
    "\"SUBJECT_NOVELTY\",\n",
    "\"OBJECT_CUI\",\n",
    "\"OBJECT_NAME\",\n",
    "\"OBJECT_SEMTYPE\",\n",
    "\"OBJECT_NOVELTY\"]\n",
    "\n",
    "df = df[[\"SUBJECT_CUI\", \"SUBJECT_NAME\",  \"PREDICATE\",\n",
    "      \"PMID\", ## drop or use agg counts for confidence?\n",
    "         # \"SUBJECT_SEMTYPE\",\"OBJECT_SEMTYPE\", ## semtype not used currently - skip it \n",
    "    \"SUBJECT_NOVELTY\", \"OBJECT_CUI\", \"OBJECT_NAME\",  \"OBJECT_NOVELTY\"]].drop_duplicates()\n",
    "print(df.shape[0])\n",
    "## ORIG: df = df.loc[df.groupby(\"PREDICATE\").transform(\"size\")>=100].copy() \n",
    "df = df.loc[df.groupby(\"PREDICATE\")[\"SUBJECT_CUI\"].transform(\"count\")>=50].reset_index(drop=True).copy() \n",
    "print(df.shape[0],\"After dropping rare predicates\")\n",
    "# ## add?\n",
    "# df = df.loc[(df.groupby(\"OBJECT_CUI\")[\"SUBJECT_CUI\"].transform(\"size\")>=2)\\\n",
    "# & (df.groupby(\"SUBJECT_CUI\")[\"OBJECT_CUI\"].transform(\"size\")>=2)].copy()\n",
    "# print(df.shape[0],\"After dropping rare CUIs\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09247b6d-d593-493b-9113-adbf5d6332b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e25186-6c6c-4fdb-a815-d7254ab20c26",
   "metadata": {},
   "source": [
    "#### multi terms not fixed yet\n",
    "* Also, may be enough to take first term per \"|\" list? Some are different terms maybe , some seem like synonyms, e.g. NPY/Neuropeptide Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf03fdc-4e77-4322-9290-66c7603cfacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at cases with \"|\" in them, in subject... \n",
    "\"\"\"\n",
    "data_updated = df.loc[df[\"SUBJECT_CUI\"].str.contains(\"|\",na=False,regex=False)]\n",
    "display(data_updated)\n",
    "\n",
    "# Split the SUBJECT_CUI and SUBJECT_NAME columns, handling NaN values appropriately\n",
    "split_cui = data_updated['SUBJECT_CUI'].str.split('|').apply(lambda x: x if isinstance(x, list) else [])\n",
    "split_name = data_updated['SUBJECT_NAME'].str.split('|').apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Explode the DataFrame, preserving rows even if they have empty lists from NaN entries\n",
    "exploded_data_updated = data_updated.loc[data_updated.index.repeat(split_cui.apply(len))].copy()\n",
    "exploded_data_updated['SUBJECT_CUI'] = pd.Series([item for sublist in split_cui for item in sublist])\n",
    "exploded_data_updated['SUBJECT_NAME'] = pd.Series([item for sublist in split_name for item in sublist])\n",
    "\n",
    "# Function to adjust CUIs, handling missing or incorrect values\n",
    "def correct_adjust_cui(cui):\n",
    "    if pd.isna(cui):\n",
    "        return cui  # Keep NaN as NaN\n",
    "    if not str(cui).startswith('C'):\n",
    "        return 'C' + cui\n",
    "    return cui\n",
    "\n",
    "# Apply the corrected CUI function to ensure all CUIs are correctly formatted\n",
    "exploded_data_updated['SUBJECT_CUI'] = exploded_data_updated['SUBJECT_CUI'].apply(correct_adjust_cui)\n",
    "\n",
    "# Drop any duplicates and reset index\n",
    "final_data_updated = exploded_data_updated.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Show the updated DataFrame and check for NaN handling\n",
    "final_data_updated\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34996279-8f74-401e-b4cb-0d071f78f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cite.shape[0])\n",
    "df_cite = df[[\"PMID\",\"SUBJECT_CUI\",\"OBJECT_CUI\",\"PREDICATE\"]].drop_duplicates().merge(df_cite,on=\"PMID\")\n",
    "print(df_cite.shape[0])\n",
    "print(df_cite.nunique())\n",
    "### Get earliest year of a SVO occurring, or a pair occurring \n",
    "\n",
    "df_cite[\"first_year_pair\"] = df_cite.groupby([\"SUBJECT_CUI\",\"OBJECT_CUI\"])[\"YEAR\"].transform(\"min\")\n",
    "df_cite[\"first_year_triple\"] = df_cite.groupby([\"SUBJECT_CUI\",\"OBJECT_CUI\",\"PREDICATE\"])[\"YEAR\"].transform(\"min\")\n",
    "\n",
    "if SAVE:\n",
    "    df_cite.to_parquet(\"kg_first_cite_history_raw.parquet\")\n",
    "df_cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869face-d468-4aaf-9465-ccb4dd87d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cite.groupby([\"SUBJECT_CUI\",\"OBJECT_CUI\"])[\"YEAR\"].transform(\"nunique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f44ad8-2861-47c0-8eda-f013ccceeac9",
   "metadata": {},
   "source": [
    "### drop rareties: S-O level (pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0dc5c-974e-4605-b59b-d7e8d1686516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_cite[[\"SUBJECT_CUI\",\"OBJECT_CUI\",\"PMID\"]].drop_duplicates().copy()\n",
    "df2[\"pair_counts\"] = df2.groupby([\"SUBJECT_CUI\",\"OBJECT_CUI\"],observed=True)[\"PMID\"].transform(\"nunique\")\n",
    "\n",
    "df2 = df2.drop(columns=[\"PMID\"]).drop_duplicates()\n",
    "print(df2.shape[0],\"# pairwise count rows of subject/object\")\n",
    "\n",
    "df2 = df2.loc[df2[\"pair_counts\"]>=FILTER_MIN_PAIR_COOC_THRESHHOLD]\n",
    "print(df2.shape[0],\"# pairwise count rows of subject/object, with >1 occurence\")\n",
    "# df2[\"pair_counts\"] = df2.groupby([\"SUBJECT_CUI\",\"OBJECT_CUI\"],observed=True)[\"PMID\"].nunique()\n",
    "\n",
    "s1 = df_cite.shape[0]\n",
    "print(s1,\"prev df_cite size\")\n",
    "df_cite = df_cite.merge(df2,on=[\"SUBJECT_CUI\",\"OBJECT_CUI\"],how=\"inner\")\n",
    "assert s1>= df_cite.shape[0] ,df_cite.shape[0]\n",
    "print(df_cite.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcdc41c-0220-489f-991e-e3f253f86a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "df = df.merge(df_cite[[\"SUBJECT_CUI\",\"OBJECT_CUI\",\"PREDICATE\",\"first_year_pair\",\"first_year_triple\",\"pair_counts\"]].drop_duplicates(),\n",
    "             on=[\"SUBJECT_CUI\",\"OBJECT_CUI\",\"PREDICATE\"])\n",
    "print(df.shape[0])\n",
    "\n",
    "# ## add new - pairwise counts (ignores predicate). \n",
    "# ## Could maybe not count \"NEG_\" predicates for this purpose? \n",
    "# df[\"pair_counts\"] = df.groupby([\"SUBJECT_CUI\",\"OBJECT_CUI\"],observed=True)[\"SUBJECT_NOVELTY\"].transform(\"size\")\n",
    "del df_cite, df2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c7fed-1c3c-41c2-bc03-45cb1b99a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.merge(df2,on=[\"SUBJECT_CUI\",\"OBJECT_CUI\"]) ## add pair_counts - # unique papers a pair appeared in. \n",
    "# print(df.shape[0])\n",
    "# del df_cite, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895b65f-7ca6-456e-9557-a0d7d27f5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(df.shape[0])\n",
    "df = df.drop_duplicates().drop(columns=[\"PMID\"]) # 1 case max per work - Now will also need dropping years..\n",
    "\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad54522-93aa-4e20-ba68-77fc00db8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7f6b8-694e-45dc-abad-205cae2b09ea",
   "metadata": {},
   "source": [
    "* Q: Note: more names than unique CUIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087824c-afd7-4c50-b7e9-0a16e2fc100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a66ca-caca-4e66-960b-093f3f24dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (new) comment out here, we do it anyway below \n",
    "# for c in df.select_dtypes(\"O\").columns:\n",
    "#     df[c] = df[c].astype(\"category\")\n",
    "df[\"OBJECT_NOVELTY\"] = df[\"OBJECT_NOVELTY\"].astype(bool)\n",
    "df[\"SUBJECT_NOVELTY\"] = df[\"SUBJECT_NOVELTY\"].astype(bool)\n",
    "df.info()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11089d3-3323-4a36-95a5-0e53be66b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9e016-22de-4ffd-b791-e1447358d9e0",
   "metadata": {},
   "source": [
    "##### Record # occurrences per triple, and keep max - at `SVO/triple` level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2486a0b-4dc5-448e-9fca-4c41d65caa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# may be very slow\n",
    "##\n",
    "df[\"counts\"] = df.groupby([\"SUBJECT_CUI\",\"PREDICATE\",\"OBJECT_CUI\"],observed=True)[\"SUBJECT_NOVELTY\"].transform(\"size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a976a7d-d415-4ad4-942b-946879d1aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.sort_values([\"counts\",\"SUBJECT_CUI\",\"PREDICATE\",\"OBJECT_CUI\"],ascending=False).drop_duplicates(subset=[\"PREDICATE\",\n",
    "                                                                                                               \"SUBJECT_CUI\",\"OBJECT_CUI\",\n",
    "                                                                                                               \"SUBJECT_NAME\",\"OBJECT_NAME\",\n",
    "                                                                                                               # \"SUBJECT_NOVELTY\",\"OBJECT_NOVELTY\"\n",
    "                                                                                                              ])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea18867-116e-4629-aeae-29b50c36e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dac78e-7764-42ee-8ded-96f655e15145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PREDICATE\"].value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eccf94-3986-454e-842c-526b1fdf937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PREDICATE\"].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24113628-1191-4243-8942-2b4d1447beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"SUBJECT_CUI\"].str.contains(r\"|\",case=False,regex=False)].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139401dd-2c16-4847-885a-ffd3f9af6756",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## new - \n",
    "##  ||| gene objecgts (that require exploding ) - handle them here in advance\n",
    "splitter_mask = (df[\"SUBJECT_CUI\"].str.contains(r\"|\",case=False,regex=False)) | (df[\"OBJECT_CUI\"].str.contains(r\"|\",case=False,regex=False))\n",
    "for col in df.select_dtypes(include=['category']).columns:\n",
    "  df[col] = df[col].astype('str')\n",
    "    \n",
    "df.loc[splitter_mask,\"SUBJECT_NAME\"] = df.loc[splitter_mask][\"SUBJECT_NAME\"].str.split(r\"|\",regex=False,expand=True)[0]\n",
    "df.loc[splitter_mask,\"OBJECT_NAME\"] = df.loc[splitter_mask][\"OBJECT_NAME\"].str.split(r\"|\",regex=False,expand=True)[0]\n",
    "\n",
    "df.loc[splitter_mask,\"SUBJECT_CUI\"] = df.loc[splitter_mask][\"SUBJECT_CUI\"].str.split(r\"|\",regex=False,expand=True)[0]\n",
    "df.loc[splitter_mask,\"OBJECT_CUI\"] = df.loc[splitter_mask][\"OBJECT_CUI\"].str.split(r\"|\",regex=False,expand=True)[0]\n",
    "# duplicate of doing this above..\n",
    "for c in df.select_dtypes(\"O\").columns:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "# df[\"OBJECT_NOVELTY\"] = df[\"OBJECT_NOVELTY\"].astype(bool)\n",
    "# df[\"SUBJECT_NOVELTY\"] = df[\"SUBJECT_NOVELTY\"].astype(bool)\n",
    "df.drop(columns=[\"OBJECT_NOVELTY\",\"SUBJECT_NOVELTY\",\"SUBJECT_SEMTYPE\",\"OBJECT_SEMTYPE\"],errors=\"ignore\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25f73b-f89f-4ad3-9206-2d13949afb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df[\"SUBJECT_CUI\"].str.contains(r\"|\",case=False,regex=False)].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d51440-3330-4025-90ba-6ced83dc1921",
   "metadata": {},
   "source": [
    "#### keep CUIs that appear at least 2 times in data/KG - per source cui?\n",
    "#### Drop rare predicates\n",
    "* Drops tiny (~<1%) amount of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfc95b-b7ef-4cea-8cce-7ac300cb2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(df.shape[0])\n",
    "print(df[\"PREDICATE\"].nunique())\n",
    "\n",
    "df = df.loc[(df.groupby(\"OBJECT_CUI\")[\"SUBJECT_CUI\"].transform(\"size\")>=2)\\\n",
    "& (df.groupby(\"SUBJECT_CUI\")[\"OBJECT_CUI\"].transform(\"size\")>=2)].copy()\n",
    "print(df.shape[0],\"# rows after dropping singleton entities\")\n",
    "\n",
    "## pair counts filtering already done\n",
    "## new - drop cases of a pair appearing only 1 time, regardless of predicate\n",
    "df = df.loc[df[\"pair_counts\"]>1]\n",
    "print(df.shape[0],\"# rows after dropping singleton pair counts\")\n",
    "\n",
    "## drop super rare/noise predicates\n",
    "df = df.loc[df.groupby(\"PREDICATE\").transform(\"size\")>=50]#.copy() # 27,350,365  - very few cases of bad predicates\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "for c in df.select_dtypes(\"category\").columns:\n",
    "# remove unobserved categories, in new filtered data\n",
    "    df[c] = df[c].cat.remove_unused_categories()\n",
    "print(df.shape[0])\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc2ef6-8ba3-46f2-99c2-f449ea7c00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b30839-9d23-4869-b785-87eb17200dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if SAVE:\n",
    "    df.to_parquet(\"predications.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0136524-480c-4636-a19b-490e99441f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"predications.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9733f9-500a-4e8a-9ea9-cebb72bdd128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"counts>2 & first_year_pair>1948\")[\"first_year_pair\"].hist()\n",
    "df.query(\"counts>2 & first_year_pair>1948\")[\"first_year_pair\"].describe().round()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
